{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPgm6S8Ofn9DvJpi5wo+d9s"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Occupational Status Prediction using Machine Learning\n",
        "\n",
        "This project implements a multi-model AI system to predict an individual's occupational status based on demographic, educational, and linguistic features. The system was developed using data from the 2022 Adult Education Survey (AEDA2022), published by the Romanian National Institute of Statistics, and was used in the National Statistics Olympiad by the Author.\n",
        "\n",
        "## Objective\n",
        "\n",
        "The primary goal of this project is to analyze how factors such as formal education, non-formal learning, and language usage influence labor market integration. The model predicts categories such as \"Employed\", \"Unemployed\", \"Student\", \"Retired\", and others, using a wide range of supervised learning algorithms.\n",
        "\n",
        "## Methodology\n",
        "\n",
        "A class-based machine learning pipeline was implemented to ensure clean design, reproducibility, and easy extensibility. The pipeline includes preprocessing steps such as standardization and one-hot encoding, followed by individual model training and ensemble learning.\n",
        "\n",
        "### Models Used\n",
        "\n",
        "The project applies and compares multiple machine learning models suitable for structured/tabular data:\n",
        "\n",
        "- XGBoost Classifier\n",
        "- Random Forest Classifier\n",
        "- Support Vector Machine (SVC)\n",
        "- Logistic Regression\n",
        "- Gradient Boosting Classifier\n",
        "- AdaBoost Classifier\n",
        "\n",
        "Each model is trained independently and evaluated based on classification performance. A final **Stacked Ensemble Model** combines the strengths of all base learners to improve robustness and predictive accuracy.\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "The system evaluates each model using the following metrics:\n",
        "\n",
        "- Accuracy\n",
        "- Precision\n",
        "- Recall\n",
        "- F1 Score\n",
        "- Confusion Matrix\n",
        "- Classification Report\n",
        "\n",
        "The stacked model serves as the final predictor, leveraging the combined insights of all models.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "- **Name**: Ancheta asupra Educației Adulților (AEDA2022)\n",
        "- **Publisher**: Institutul Național de Statistică (INSSE), România\n",
        "- **Official Report**: https://insse.ro/cms/files/Rapoarte%20de%20calitate/Educatie/Ancheta-asupra-educatiei-adultilor.pdf\n",
        "\n",
        "## Author\n",
        "\n",
        "- **Name**: Tihoc Andrei  \n",
        "\n",
        "## Potential Enhancements\n",
        "\n",
        "- Hyperparameter tuning with GridSearchCV or Optuna\n",
        "- SHAP-based model explainability\n",
        "- Frontend deployment using Streamlit or FastAPI\n",
        "- Integration with real-time systems or APIs\n",
        "\n",
        "## Disclaimer\n",
        "\n",
        "This project is provided for **educational and research purposes only**.  \n",
        "It is not intended to be used in real-world decision-making processes, such as hiring, profiling, or eligibility assessments.  \n",
        "Predictions are based on publicly available, anonymized data and should not be interpreted as recommendations for individuals.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "gjUbyF30NUfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install optuna\n",
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZYMAsIYSJsr",
        "outputId": "08d01078-5dc5-4e2a-e23b-192b9faabafb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all required libraries\n",
        "# This cell imports all key libraries needed for data manipulation, visualization,\n",
        "# model building, preprocessing, hyperparameter tuning, and evaluation.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import optuna\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ],
      "metadata": {
        "id": "IZkBuhd5SOcr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing and modeling pipeline class\n",
        "# This class encapsulates all preprocessing, encoding, model training, and ensemble logic.\n",
        "\n",
        "class OccupationalStatusModel:\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.scaler = StandardScaler()\n",
        "        self.pipeline = None\n",
        "        self.models = {}\n",
        "        self.trained_models = {}\n",
        "        self.feature_names = None\n",
        "\n",
        "    def preprocess(self):\n",
        "        mappings = self.get_mappings()\n",
        "        self.df['MAINSTAT'] = self.df['MAINSTAT'].map(mappings['mainstat'])\n",
        "        self.df['HATLEVEL 1'] = self.df['HATLEVEL 1'].map(mappings['hatlevel'])\n",
        "        self.df['HATFIELDniv1'] = self.df['HATFIELDniv1'].astype(str).map(mappings['hatfield'])\n",
        "\n",
        "    def get_mappings(self):\n",
        "        return {\n",
        "            \"mainstat\": {\n",
        "                10: \"Employed\", 20: \"Unemployed\", 32: \"Retired\", 33: \"Disabled\",\n",
        "                31: \"Student\", 35: \"Housework\", 34: \"Military\", 36: \"Other\"\n",
        "            },\n",
        "            \"hatlevel\": {\n",
        "                100: \"Primary\", 200: \"Lower Secondary\", 342: \"Upper Secondary (partial)\",\n",
        "                343: \"Upper Secondary (no tertiary access)\", 344: \"Upper Secondary (tertiary access)\",\n",
        "                349: \"Upper Secondary (no access)\", 352: \"VET (partial)\", 353: \"VET (no tertiary access)\",\n",
        "                354: \"VET (tertiary access)\", 359: \"VET (no access)\", 392: \"Unknown Secondary (partial)\",\n",
        "                393: \"Unknown Secondary (no tertiary)\", 394: \"Unknown Secondary (tertiary)\",\n",
        "                399: \"Unknown Secondary (no access)\", 440: \"Post-secondary general\",\n",
        "                450: \"Post-secondary vocational\", 490: \"Post-secondary unknown\",\n",
        "                540: \"Short Tertiary (general)\", 550: \"Short Tertiary (vocational)\",\n",
        "                590: \"Short Tertiary (unknown)\", 600: \"Bachelor\", 700: \"Master\", 800: \"Doctorate\"\n",
        "            },\n",
        "            \"hatfield\": {\n",
        "                \"1\": \"Education\", \"2\": \"Arts and Humanities\", \"3\": \"Social Sciences\",\n",
        "                \"4\": \"Business and Law\", \"5\": \"Science and Math\", \"6\": \"ICT\",\n",
        "                \"7\": \"Engineering\", \"8\": \"Agriculture\", \"9\": \"Health\", \"10\": \"Services\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def prepare_data(self):\n",
        "        features = ['VARSTA', 'HATLEVEL 1', 'FEDLEVEL', 'NFELESSON', 'NFEWORKSHOP', 'LANGUSED']\n",
        "        X = self.df[features]\n",
        "        y = self.df['MAINSTAT']\n",
        "\n",
        "        categorical = ['HATLEVEL 1', 'FEDLEVEL']\n",
        "        numerical = ['VARSTA', 'NFELESSON', 'NFEWORKSHOP', 'LANGUSED']\n",
        "\n",
        "        preprocessor = ColumnTransformer([\n",
        "            ('num', self.scaler, numerical),\n",
        "            ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical)\n",
        "        ])\n",
        "\n",
        "        self.pipeline = Pipeline([\n",
        "            ('preprocessor', preprocessor)\n",
        "        ])\n",
        "\n",
        "        y_encoded = self.label_encoder.fit_transform(y)\n",
        "        X_processed = self.pipeline.fit_transform(X)\n",
        "        self.feature_names = self.pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
        "\n",
        "        return train_test_split(X_processed, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n",
        "\n",
        "    def optimize_model(self, model_name, X_train, y_train):\n",
        "        def objective(trial):\n",
        "            if model_name == 'xgb':\n",
        "                model = XGBClassifier(\n",
        "                    max_depth=trial.suggest_int(\"max_depth\", 3, 10),\n",
        "                    learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "                    n_estimators=trial.suggest_int(\"n_estimators\", 50, 200),\n",
        "                    subsample=trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "                    use_label_encoder=False,\n",
        "                    eval_metric='mlogloss',\n",
        "                    random_state=42\n",
        "                )\n",
        "            elif model_name == 'rf':\n",
        "                model = RandomForestClassifier(\n",
        "                    n_estimators=trial.suggest_int(\"n_estimators\", 50, 200),\n",
        "                    max_depth=trial.suggest_int(\"max_depth\", 3, 15),\n",
        "                    min_samples_split=trial.suggest_int(\"min_samples_split\", 2, 10),\n",
        "                    min_samples_leaf=trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
        "                    random_state=42\n",
        "                )\n",
        "            elif model_name == 'svm':\n",
        "                model = SVC(\n",
        "                    C=trial.suggest_float(\"C\", 0.1, 10.0),\n",
        "                    kernel='linear',\n",
        "                    probability=True,\n",
        "                    random_state=42\n",
        "                )\n",
        "            elif model_name == 'logreg':\n",
        "                model = LogisticRegression(\n",
        "                    C=trial.suggest_float(\"C\", 0.01, 10.0),\n",
        "                    max_iter=1000\n",
        "                )\n",
        "            elif model_name == 'gb':\n",
        "                model = GradientBoostingClassifier(\n",
        "                    n_estimators=trial.suggest_int(\"n_estimators\", 50, 200),\n",
        "                    learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "                    max_depth=trial.suggest_int(\"max_depth\", 3, 10),\n",
        "                    random_state=42\n",
        "                )\n",
        "            elif model_name == 'ada':\n",
        "                model = AdaBoostClassifier(\n",
        "                    n_estimators=trial.suggest_int(\"n_estimators\", 50, 200),\n",
        "                    learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 1.0),\n",
        "                    random_state=42\n",
        "                )\n",
        "            else:\n",
        "                return 0.0\n",
        "\n",
        "            model.fit(X_train, y_train)\n",
        "            preds = model.predict(X_train)\n",
        "            return accuracy_score(y_train, preds)\n",
        "\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=30, show_progress_bar=False)\n",
        "        print(f\"Best trial for {model_name}: {study.best_trial.params}\")\n",
        "        return objective(study.best_trial), study.best_trial.params\n",
        "\n",
        "    def train_models(self, X_train, y_train):\n",
        "        self.models = {\n",
        "            'xgb': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
        "            'rf': RandomForestClassifier(random_state=42),\n",
        "            'svm': SVC(kernel='linear', probability=True, random_state=42),\n",
        "            'logreg': LogisticRegression(max_iter=1000),\n",
        "            'gb': GradientBoostingClassifier(random_state=42),\n",
        "            'ada': AdaBoostClassifier(random_state=42)\n",
        "        }\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            _, best_params = self.optimize_model(name, X_train, y_train)\n",
        "            model.set_params(**best_params)\n",
        "            print(f\"\\nTraining {name.upper()}...\")\n",
        "            model.fit(X_train, y_train)\n",
        "            self.trained_models[name] = model\n",
        "\n",
        "    def evaluate_models(self, X_test, y_test):\n",
        "        results = {}\n",
        "        for name, model in self.trained_models.items():\n",
        "            y_pred = model.predict(X_test)\n",
        "            acc = accuracy_score(y_test, y_pred)\n",
        "            prec = precision_score(y_test, y_pred, average='weighted')\n",
        "            rec = recall_score(y_test, y_pred, average='weighted')\n",
        "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "            print(f\"\\n{name.upper()} Results:\")\n",
        "            print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
        "            print(classification_report(y_test, y_pred, target_names=self.label_encoder.classes_))\n",
        "            results[name] = acc\n",
        "        return results\n",
        "\n",
        "    def build_stacked_model(self):\n",
        "        estimators = [(name, model) for name, model in self.trained_models.items() if name != 'logreg']\n",
        "        stacked = StackingClassifier(\n",
        "            estimators=estimators,\n",
        "            final_estimator=LogisticRegression(max_iter=1000),\n",
        "            passthrough=True,\n",
        "            cv=5\n",
        "        )\n",
        "        return stacked\n",
        "\n",
        "    def predict_stacked(self, stacked_model, X_train, X_test, y_train, y_test):\n",
        "        stacked_model.fit(X_train, y_train)\n",
        "        y_pred = stacked_model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        print(\"\\nSTACKED MODEL RESULTS\")\n",
        "        print(f\"Accuracy: {acc:.4f}\")\n",
        "        print(classification_report(y_test, y_pred, target_names=self.label_encoder.classes_))\n",
        "        return y_pred\n",
        "\n"
      ],
      "metadata": {
        "id": "hJ_r1_u0VF8L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Excel data\n",
        "file_path = \"Olimpiada Martie 2025.xlsx\"\n",
        "xls = pd.ExcelFile(file_path)\n",
        "df = pd.read_excel(xls, sheet_name=\"Date_seniori\")"
      ],
      "metadata": {
        "id": "8RdBp5XzVIC6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and run model\n",
        "model = OccupationalStatusModel(df)\n",
        "model.preprocess()\n",
        "X_train, X_test, y_train, y_test = model.prepare_data()\n",
        "model.train_models(X_train, y_train)\n",
        "results = model.evaluate_models(X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rfrZktbVPOs",
        "outputId": "1b89cc47-a69d-4cff-e1b3-c7d33b4c51ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-08 07:45:59,288] A new study created in memory with name: no-name-3b9a71f7-8ec0-4d47-8488-4af2f0dcb01f\n",
            "[I 2025-04-08 07:46:01,187] Trial 0 finished with value: 0.8604166666666667 and parameters: {'max_depth': 7, 'learning_rate': 0.09814729608847703, 'n_estimators': 127, 'subsample': 0.9691734896715636}. Best is trial 0 with value: 0.8604166666666667.\n",
            "[I 2025-04-08 07:46:01,646] Trial 1 finished with value: 0.8410416666666667 and parameters: {'max_depth': 5, 'learning_rate': 0.03516059767654904, 'n_estimators': 91, 'subsample': 0.8625579367927099}. Best is trial 0 with value: 0.8604166666666667.\n",
            "[I 2025-04-08 07:46:03,179] Trial 2 finished with value: 0.8675 and parameters: {'max_depth': 10, 'learning_rate': 0.14658763397025557, 'n_estimators': 169, 'subsample': 0.5040905117875293}. Best is trial 2 with value: 0.8675.\n",
            "[I 2025-04-08 07:46:03,739] Trial 3 finished with value: 0.8427083333333333 and parameters: {'max_depth': 5, 'learning_rate': 0.050451452347426616, 'n_estimators': 107, 'subsample': 0.5928898727656295}. Best is trial 2 with value: 0.8675.\n",
            "[I 2025-04-08 07:46:05,252] Trial 4 finished with value: 0.8714583333333333 and parameters: {'max_depth': 10, 'learning_rate': 0.18807696817103345, 'n_estimators': 162, 'subsample': 0.5645365493115662}. Best is trial 4 with value: 0.8714583333333333.\n",
            "[I 2025-04-08 07:46:06,430] Trial 5 finished with value: 0.8704166666666666 and parameters: {'max_depth': 6, 'learning_rate': 0.24981705728783796, 'n_estimators': 167, 'subsample': 0.9114998425983306}. Best is trial 4 with value: 0.8714583333333333.\n",
            "[I 2025-04-08 07:46:07,028] Trial 6 finished with value: 0.8541666666666666 and parameters: {'max_depth': 4, 'learning_rate': 0.20355046425914772, 'n_estimators': 136, 'subsample': 0.791776772424632}. Best is trial 4 with value: 0.8714583333333333.\n",
            "[I 2025-04-08 07:46:07,898] Trial 7 finished with value: 0.871875 and parameters: {'max_depth': 10, 'learning_rate': 0.19884555706317625, 'n_estimators': 99, 'subsample': 0.9935616978673452}. Best is trial 7 with value: 0.871875.\n",
            "[I 2025-04-08 07:46:08,613] Trial 8 finished with value: 0.8604166666666667 and parameters: {'max_depth': 5, 'learning_rate': 0.2160401925999995, 'n_estimators': 129, 'subsample': 0.7149055245021182}. Best is trial 7 with value: 0.871875.\n",
            "[I 2025-04-08 07:46:09,492] Trial 9 finished with value: 0.84625 and parameters: {'max_depth': 4, 'learning_rate': 0.07623115098638202, 'n_estimators': 187, 'subsample': 0.6560669429545545}. Best is trial 7 with value: 0.871875.\n",
            "[I 2025-04-08 07:46:09,985] Trial 10 finished with value: 0.8658333333333333 and parameters: {'max_depth': 8, 'learning_rate': 0.2894835901601538, 'n_estimators': 56, 'subsample': 0.9845307064725373}. Best is trial 7 with value: 0.871875.\n",
            "[I 2025-04-08 07:46:11,024] Trial 11 finished with value: 0.8622916666666667 and parameters: {'max_depth': 10, 'learning_rate': 0.14461223020984906, 'n_estimators': 79, 'subsample': 0.5343755809089216}. Best is trial 7 with value: 0.871875.\n",
            "[I 2025-04-08 07:46:19,252] Trial 12 finished with value: 0.873125 and parameters: {'max_depth': 9, 'learning_rate': 0.19217474984521066, 'n_estimators': 153, 'subsample': 0.8204081740304427}. Best is trial 12 with value: 0.873125.\n",
            "[I 2025-04-08 07:46:26,620] Trial 13 finished with value: 0.8729166666666667 and parameters: {'max_depth': 8, 'learning_rate': 0.2506924724506969, 'n_estimators': 144, 'subsample': 0.8135578597357926}. Best is trial 12 with value: 0.873125.\n",
            "[I 2025-04-08 07:46:33,088] Trial 14 finished with value: 0.8735416666666667 and parameters: {'max_depth': 8, 'learning_rate': 0.29731774145434936, 'n_estimators': 152, 'subsample': 0.8060440709409435}. Best is trial 14 with value: 0.8735416666666667.\n",
            "[I 2025-04-08 07:46:37,803] Trial 15 finished with value: 0.874375 and parameters: {'max_depth': 8, 'learning_rate': 0.2939573533335359, 'n_estimators': 193, 'subsample': 0.742255169224019}. Best is trial 15 with value: 0.874375.\n",
            "[I 2025-04-08 07:46:39,766] Trial 16 finished with value: 0.8741666666666666 and parameters: {'max_depth': 8, 'learning_rate': 0.2974552393979159, 'n_estimators': 196, 'subsample': 0.7194977633647702}. Best is trial 15 with value: 0.874375.\n",
            "[I 2025-04-08 07:46:41,370] Trial 17 finished with value: 0.8722916666666667 and parameters: {'max_depth': 7, 'learning_rate': 0.26047644346661786, 'n_estimators': 200, 'subsample': 0.7101238054811756}. Best is trial 15 with value: 0.874375.\n",
            "[I 2025-04-08 07:46:43,146] Trial 18 finished with value: 0.87375 and parameters: {'max_depth': 9, 'learning_rate': 0.2765828829162327, 'n_estimators': 190, 'subsample': 0.6438864819210705}. Best is trial 15 with value: 0.874375.\n",
            "[I 2025-04-08 07:46:44,396] Trial 19 finished with value: 0.8685416666666667 and parameters: {'max_depth': 6, 'learning_rate': 0.23075723501876533, 'n_estimators': 181, 'subsample': 0.7396051563429217}. Best is trial 15 with value: 0.874375.\n",
            "[I 2025-04-08 07:46:45,112] Trial 20 finished with value: 0.8495833333333334 and parameters: {'max_depth': 3, 'learning_rate': 0.2973386982224633, 'n_estimators': 177, 'subsample': 0.6615004741302464}. Best is trial 15 with value: 0.874375.\n",
            "[I 2025-04-08 07:46:46,947] Trial 21 finished with value: 0.8739583333333333 and parameters: {'max_depth': 9, 'learning_rate': 0.26478394582150366, 'n_estimators': 200, 'subsample': 0.6567093733004287}. Best is trial 15 with value: 0.874375.\n",
            "[I 2025-04-08 07:46:49,351] Trial 22 finished with value: 0.874375 and parameters: {'max_depth': 9, 'learning_rate': 0.266096137977736, 'n_estimators': 200, 'subsample': 0.6879329400650536}. Best is trial 15 with value: 0.874375.\n",
            "[I 2025-04-08 07:46:51,984] Trial 23 finished with value: 0.87375 and parameters: {'max_depth': 8, 'learning_rate': 0.23408528297343428, 'n_estimators': 191, 'subsample': 0.760367728545631}. Best is trial 15 with value: 0.874375.\n",
            "[I 2025-04-08 07:46:53,411] Trial 24 finished with value: 0.8714583333333333 and parameters: {'max_depth': 7, 'learning_rate': 0.2757071166059324, 'n_estimators': 177, 'subsample': 0.6978510969052425}. Best is trial 15 with value: 0.874375.\n",
            "[I 2025-04-08 07:46:55,243] Trial 25 finished with value: 0.8739583333333333 and parameters: {'max_depth': 9, 'learning_rate': 0.23689488093335107, 'n_estimators': 200, 'subsample': 0.6213405554714988}. Best is trial 15 with value: 0.874375.\n",
            "[I 2025-04-08 07:46:56,644] Trial 26 finished with value: 0.866875 and parameters: {'max_depth': 8, 'learning_rate': 0.11636013468132761, 'n_estimators': 161, 'subsample': 0.763446127209559}. Best is trial 15 with value: 0.874375.\n",
            "[I 2025-04-08 07:46:57,720] Trial 27 finished with value: 0.8704166666666666 and parameters: {'max_depth': 9, 'learning_rate': 0.17081992007869806, 'n_estimators': 116, 'subsample': 0.8628937727792783}. Best is trial 15 with value: 0.874375.\n",
            "[I 2025-04-08 07:46:59,106] Trial 28 finished with value: 0.8716666666666667 and parameters: {'max_depth': 7, 'learning_rate': 0.2770167045040912, 'n_estimators': 173, 'subsample': 0.6981944596086314}. Best is trial 15 with value: 0.874375.\n",
            "[I 2025-04-08 07:47:00,618] Trial 29 finished with value: 0.8747916666666666 and parameters: {'max_depth': 7, 'learning_rate': 0.29708769890255043, 'n_estimators': 188, 'subsample': 0.933169452251674}. Best is trial 29 with value: 0.8747916666666666.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial for xgb: {'max_depth': 7, 'learning_rate': 0.29708769890255043, 'n_estimators': 188, 'subsample': 0.933169452251674}\n",
            "\n",
            "Training XGB...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-08 07:47:04,714] A new study created in memory with name: no-name-309f1e76-fe5e-4e1d-a5c5-c36953c533fc\n",
            "[I 2025-04-08 07:47:05,455] Trial 0 finished with value: 0.8347916666666667 and parameters: {'n_estimators': 180, 'max_depth': 11, 'min_samples_split': 9, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.8347916666666667.\n",
            "[I 2025-04-08 07:47:06,093] Trial 1 finished with value: 0.8360416666666667 and parameters: {'n_estimators': 136, 'max_depth': 12, 'min_samples_split': 3, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.8360416666666667.\n",
            "[I 2025-04-08 07:47:06,592] Trial 2 finished with value: 0.8352083333333333 and parameters: {'n_estimators': 119, 'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.8360416666666667.\n",
            "[I 2025-04-08 07:47:07,003] Trial 3 finished with value: 0.8360416666666667 and parameters: {'n_estimators': 88, 'max_depth': 11, 'min_samples_split': 10, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.8360416666666667.\n",
            "[I 2025-04-08 07:47:07,264] Trial 4 finished with value: 0.816875 and parameters: {'n_estimators': 99, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.8360416666666667.\n",
            "[I 2025-04-08 07:47:07,726] Trial 5 finished with value: 0.8316666666666667 and parameters: {'n_estimators': 150, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.8360416666666667.\n",
            "[I 2025-04-08 07:47:08,299] Trial 6 finished with value: 0.8314583333333333 and parameters: {'n_estimators': 184, 'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.8360416666666667.\n",
            "[I 2025-04-08 07:47:08,565] Trial 7 finished with value: 0.8333333333333334 and parameters: {'n_estimators': 69, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.8360416666666667.\n",
            "[I 2025-04-08 07:47:08,972] Trial 8 finished with value: 0.8147916666666667 and parameters: {'n_estimators': 154, 'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 1 with value: 0.8360416666666667.\n",
            "[I 2025-04-08 07:47:09,346] Trial 9 finished with value: 0.8327083333333334 and parameters: {'n_estimators': 117, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.8360416666666667.\n",
            "[I 2025-04-08 07:47:09,634] Trial 10 finished with value: 0.8375 and parameters: {'n_estimators': 52, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 10 with value: 0.8375.\n",
            "[I 2025-04-08 07:47:09,941] Trial 11 finished with value: 0.836875 and parameters: {'n_estimators': 55, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 10 with value: 0.8375.\n",
            "[I 2025-04-08 07:47:10,248] Trial 12 finished with value: 0.836875 and parameters: {'n_estimators': 55, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 10 with value: 0.8375.\n",
            "[I 2025-04-08 07:47:10,636] Trial 13 finished with value: 0.8464583333333333 and parameters: {'n_estimators': 54, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8464583333333333.\n",
            "[I 2025-04-08 07:47:11,269] Trial 14 finished with value: 0.8527083333333333 and parameters: {'n_estimators': 81, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 14 with value: 0.8527083333333333.\n",
            "[I 2025-04-08 07:47:11,851] Trial 15 finished with value: 0.8489583333333334 and parameters: {'n_estimators': 82, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 14 with value: 0.8527083333333333.\n",
            "[I 2025-04-08 07:47:12,476] Trial 16 finished with value: 0.85 and parameters: {'n_estimators': 86, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 14 with value: 0.8527083333333333.\n",
            "[I 2025-04-08 07:47:12,918] Trial 17 finished with value: 0.83625 and parameters: {'n_estimators': 102, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 14 with value: 0.8527083333333333.\n",
            "[I 2025-04-08 07:47:13,604] Trial 18 finished with value: 0.8435416666666666 and parameters: {'n_estimators': 77, 'max_depth': 13, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 14 with value: 0.8527083333333333.\n",
            "[I 2025-04-08 07:47:14,282] Trial 19 finished with value: 0.8360416666666667 and parameters: {'n_estimators': 105, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 14 with value: 0.8527083333333333.\n",
            "[I 2025-04-08 07:47:14,988] Trial 20 finished with value: 0.8485416666666666 and parameters: {'n_estimators': 71, 'max_depth': 13, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 14 with value: 0.8527083333333333.\n",
            "[I 2025-04-08 07:47:15,683] Trial 21 finished with value: 0.85 and parameters: {'n_estimators': 86, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 14 with value: 0.8527083333333333.\n",
            "[I 2025-04-08 07:47:16,294] Trial 22 finished with value: 0.8447916666666667 and parameters: {'n_estimators': 90, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 14 with value: 0.8527083333333333.\n",
            "[I 2025-04-08 07:47:16,722] Trial 23 finished with value: 0.835 and parameters: {'n_estimators': 109, 'max_depth': 8, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 14 with value: 0.8527083333333333.\n",
            "[I 2025-04-08 07:47:17,753] Trial 24 finished with value: 0.8516666666666667 and parameters: {'n_estimators': 132, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 14 with value: 0.8527083333333333.\n",
            "[I 2025-04-08 07:47:18,669] Trial 25 finished with value: 0.8452083333333333 and parameters: {'n_estimators': 140, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 14 with value: 0.8527083333333333.\n",
            "[I 2025-04-08 07:47:19,462] Trial 26 finished with value: 0.8379166666666666 and parameters: {'n_estimators': 166, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 14 with value: 0.8527083333333333.\n",
            "[I 2025-04-08 07:47:21,122] Trial 27 finished with value: 0.855625 and parameters: {'n_estimators': 200, 'max_depth': 14, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 27 with value: 0.855625.\n",
            "[I 2025-04-08 07:47:22,309] Trial 28 finished with value: 0.8454166666666667 and parameters: {'n_estimators': 169, 'max_depth': 14, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 27 with value: 0.855625.\n",
            "[I 2025-04-08 07:47:23,326] Trial 29 finished with value: 0.8391666666666666 and parameters: {'n_estimators': 191, 'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 27 with value: 0.855625.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial for rf: {'n_estimators': 200, 'max_depth': 14, 'min_samples_split': 4, 'min_samples_leaf': 1}\n",
            "\n",
            "Training RF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-08 07:47:26,866] A new study created in memory with name: no-name-34170c10-929e-4213-9ad8-d6339e27d6a2\n",
            "[I 2025-04-08 07:47:28,723] Trial 0 finished with value: 0.840625 and parameters: {'C': 7.393768252252545}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:30,117] Trial 1 finished with value: 0.8402083333333333 and parameters: {'C': 2.7183886942922575}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:31,514] Trial 2 finished with value: 0.8397916666666667 and parameters: {'C': 1.9203872356149405}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:32,934] Trial 3 finished with value: 0.8404166666666667 and parameters: {'C': 3.472795743336113}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:34,371] Trial 4 finished with value: 0.8404166666666667 and parameters: {'C': 2.9898711668516715}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:35,666] Trial 5 finished with value: 0.8402083333333333 and parameters: {'C': 1.1547899252830478}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:37,158] Trial 6 finished with value: 0.8402083333333333 and parameters: {'C': 6.426636379817529}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:39,165] Trial 7 finished with value: 0.84 and parameters: {'C': 6.000929273292096}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:40,756] Trial 8 finished with value: 0.8404166666666667 and parameters: {'C': 0.3557188824717984}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:42,310] Trial 9 finished with value: 0.840625 and parameters: {'C': 8.987643205262517}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:43,870] Trial 10 finished with value: 0.840625 and parameters: {'C': 9.822396741668534}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:45,430] Trial 11 finished with value: 0.840625 and parameters: {'C': 9.082526491985744}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:46,934] Trial 12 finished with value: 0.840625 and parameters: {'C': 7.8839161595887}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:48,487] Trial 13 finished with value: 0.840625 and parameters: {'C': 7.81252461453202}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:50,009] Trial 14 finished with value: 0.840625 and parameters: {'C': 7.974219200279267}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:52,165] Trial 15 finished with value: 0.84 and parameters: {'C': 4.884209225381152}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:53,847] Trial 16 finished with value: 0.8402083333333333 and parameters: {'C': 6.575503692861917}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:55,387] Trial 17 finished with value: 0.840625 and parameters: {'C': 9.06433118188294}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:57,256] Trial 18 finished with value: 0.84 and parameters: {'C': 4.739667938932247}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:47:59,607] Trial 19 finished with value: 0.840625 and parameters: {'C': 7.160757718348676}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:48:01,170] Trial 20 finished with value: 0.840625 and parameters: {'C': 9.815035988934508}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:48:02,832] Trial 21 finished with value: 0.840625 and parameters: {'C': 9.758279952094094}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:48:05,257] Trial 22 finished with value: 0.840625 and parameters: {'C': 8.83187302396521}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:48:06,811] Trial 23 finished with value: 0.840625 and parameters: {'C': 8.582020409200473}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:48:08,393] Trial 24 finished with value: 0.840625 and parameters: {'C': 9.805595819458805}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:48:09,888] Trial 25 finished with value: 0.840625 and parameters: {'C': 7.351246802559053}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:48:11,376] Trial 26 finished with value: 0.84 and parameters: {'C': 5.738506522922018}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:48:12,928] Trial 27 finished with value: 0.840625 and parameters: {'C': 8.672646646467609}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:48:14,436] Trial 28 finished with value: 0.8404166666666667 and parameters: {'C': 7.043933798624189}. Best is trial 0 with value: 0.840625.\n",
            "[I 2025-04-08 07:48:16,480] Trial 29 finished with value: 0.840625 and parameters: {'C': 8.146056141832043}. Best is trial 0 with value: 0.840625.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial for svm: {'C': 7.393768252252545}\n",
            "\n",
            "Training SVM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-08 07:48:19,771] A new study created in memory with name: no-name-a3a1baba-158d-4a66-a8ce-62e56eb58418\n",
            "[I 2025-04-08 07:48:19,964] Trial 0 finished with value: 0.839375 and parameters: {'C': 7.696389212430215}. Best is trial 0 with value: 0.839375.\n",
            "[I 2025-04-08 07:48:20,097] Trial 1 finished with value: 0.8383333333333334 and parameters: {'C': 2.9156153391409347}. Best is trial 0 with value: 0.839375.\n",
            "[I 2025-04-08 07:48:20,303] Trial 2 finished with value: 0.839375 and parameters: {'C': 9.192958085557965}. Best is trial 0 with value: 0.839375.\n",
            "[I 2025-04-08 07:48:20,476] Trial 3 finished with value: 0.839375 and parameters: {'C': 7.445299155601977}. Best is trial 0 with value: 0.839375.\n",
            "[I 2025-04-08 07:48:20,608] Trial 4 finished with value: 0.8391666666666666 and parameters: {'C': 5.769834795718776}. Best is trial 0 with value: 0.839375.\n",
            "[I 2025-04-08 07:48:20,747] Trial 5 finished with value: 0.8389583333333334 and parameters: {'C': 4.403547165627947}. Best is trial 0 with value: 0.839375.\n",
            "[I 2025-04-08 07:48:20,926] Trial 6 finished with value: 0.8391666666666666 and parameters: {'C': 7.634702909529076}. Best is trial 0 with value: 0.839375.\n",
            "[I 2025-04-08 07:48:21,066] Trial 7 finished with value: 0.84 and parameters: {'C': 5.211051051531521}. Best is trial 7 with value: 0.84.\n",
            "[I 2025-04-08 07:48:21,259] Trial 8 finished with value: 0.839375 and parameters: {'C': 9.381746061739253}. Best is trial 7 with value: 0.84.\n",
            "[I 2025-04-08 07:48:21,390] Trial 9 finished with value: 0.8385416666666666 and parameters: {'C': 3.2271201738930415}. Best is trial 7 with value: 0.84.\n",
            "[I 2025-04-08 07:48:21,469] Trial 10 finished with value: 0.8372916666666667 and parameters: {'C': 0.48572872925756894}. Best is trial 7 with value: 0.84.\n",
            "[I 2025-04-08 07:48:21,625] Trial 11 finished with value: 0.8402083333333333 and parameters: {'C': 5.834268606798109}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:21,789] Trial 12 finished with value: 0.839375 and parameters: {'C': 5.697746615156337}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:21,934] Trial 13 finished with value: 0.8389583333333334 and parameters: {'C': 4.37307343788494}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:22,047] Trial 14 finished with value: 0.8389583333333334 and parameters: {'C': 1.804511246304414}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:22,185] Trial 15 finished with value: 0.8391666666666666 and parameters: {'C': 6.400047991838303}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:22,328] Trial 16 finished with value: 0.8385416666666666 and parameters: {'C': 3.4827623003243016}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:22,505] Trial 17 finished with value: 0.8389583333333334 and parameters: {'C': 6.731348276783819}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:22,658] Trial 18 finished with value: 0.839375 and parameters: {'C': 5.214093971296381}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:22,768] Trial 19 finished with value: 0.8379166666666666 and parameters: {'C': 1.9337370086901875}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:22,914] Trial 20 finished with value: 0.8391666666666666 and parameters: {'C': 4.481255636434087}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:23,104] Trial 21 finished with value: 0.839375 and parameters: {'C': 8.39979799371326}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:23,275] Trial 22 finished with value: 0.8395833333333333 and parameters: {'C': 7.219603775784364}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:23,427] Trial 23 finished with value: 0.8391666666666666 and parameters: {'C': 6.721018158996512}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:23,594] Trial 24 finished with value: 0.8389583333333334 and parameters: {'C': 6.110611128215581}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:23,796] Trial 25 finished with value: 0.8391666666666666 and parameters: {'C': 8.416120435541703}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:23,959] Trial 26 finished with value: 0.839375 and parameters: {'C': 4.929828522419734}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:24,164] Trial 27 finished with value: 0.839375 and parameters: {'C': 9.902817059304944}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:24,341] Trial 28 finished with value: 0.84 and parameters: {'C': 7.073543433340778}. Best is trial 11 with value: 0.8402083333333333.\n",
            "[I 2025-04-08 07:48:24,542] Trial 29 finished with value: 0.839375 and parameters: {'C': 8.251849826601825}. Best is trial 11 with value: 0.8402083333333333.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial for logreg: {'C': 5.834268606798109}\n",
            "\n",
            "Training LOGREG...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-08 07:48:24,843] A new study created in memory with name: no-name-de72503f-696b-4364-83ad-71e1574f2a5e\n",
            "[I 2025-04-08 07:48:45,502] Trial 0 finished with value: 0.8760416666666667 and parameters: {'n_estimators': 158, 'learning_rate': 0.157005251956179, 'max_depth': 8}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:48:51,282] Trial 1 finished with value: 0.8758333333333334 and parameters: {'n_estimators': 57, 'learning_rate': 0.21371616875055008, 'max_depth': 7}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:48:59,234] Trial 2 finished with value: 0.875625 and parameters: {'n_estimators': 150, 'learning_rate': 0.27660168384120665, 'max_depth': 5}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:49:26,312] Trial 3 finished with value: 0.8760416666666667 and parameters: {'n_estimators': 123, 'learning_rate': 0.1813248153556131, 'max_depth': 10}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:49:39,737] Trial 4 finished with value: 0.875625 and parameters: {'n_estimators': 144, 'learning_rate': 0.10776021713911821, 'max_depth': 7}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:49:52,433] Trial 5 finished with value: 0.874375 and parameters: {'n_estimators': 185, 'learning_rate': 0.08165462397125577, 'max_depth': 6}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:50:04,004] Trial 6 finished with value: 0.8758333333333334 and parameters: {'n_estimators': 91, 'learning_rate': 0.10963572894838831, 'max_depth': 8}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:50:08,063] Trial 7 finished with value: 0.87 and parameters: {'n_estimators': 108, 'learning_rate': 0.23056835454731242, 'max_depth': 4}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:50:24,307] Trial 8 finished with value: 0.8760416666666667 and parameters: {'n_estimators': 172, 'learning_rate': 0.2075913874266625, 'max_depth': 7}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:50:41,761] Trial 9 finished with value: 0.8760416666666667 and parameters: {'n_estimators': 188, 'learning_rate': 0.20831919493325243, 'max_depth': 7}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:51:16,699] Trial 10 finished with value: 0.8722916666666667 and parameters: {'n_estimators': 156, 'learning_rate': 0.011641569502958637, 'max_depth': 10}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:51:43,092] Trial 11 finished with value: 0.8760416666666667 and parameters: {'n_estimators': 119, 'learning_rate': 0.15391324470914403, 'max_depth': 10}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:51:58,355] Trial 12 finished with value: 0.8760416666666667 and parameters: {'n_estimators': 89, 'learning_rate': 0.1574972824288173, 'max_depth': 9}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:52:20,095] Trial 13 finished with value: 0.8760416666666667 and parameters: {'n_estimators': 134, 'learning_rate': 0.16851362401929823, 'max_depth': 9}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:52:41,834] Trial 14 finished with value: 0.8758333333333334 and parameters: {'n_estimators': 169, 'learning_rate': 0.27271388927497964, 'max_depth': 9}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:53:08,204] Trial 15 finished with value: 0.8758333333333334 and parameters: {'n_estimators': 120, 'learning_rate': 0.051110930876595725, 'max_depth': 10}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:53:13,270] Trial 16 finished with value: 0.8595833333333334 and parameters: {'n_estimators': 199, 'learning_rate': 0.1307511421870247, 'max_depth': 3}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:53:25,685] Trial 17 finished with value: 0.8758333333333334 and parameters: {'n_estimators': 100, 'learning_rate': 0.18105795621409598, 'max_depth': 8}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:53:34,472] Trial 18 finished with value: 0.8758333333333334 and parameters: {'n_estimators': 71, 'learning_rate': 0.24384227367094163, 'max_depth': 8}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:53:57,708] Trial 19 finished with value: 0.8760416666666667 and parameters: {'n_estimators': 134, 'learning_rate': 0.1868617495516817, 'max_depth': 9}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:54:08,770] Trial 20 finished with value: 0.875625 and parameters: {'n_estimators': 160, 'learning_rate': 0.13582268944526243, 'max_depth': 6}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:54:17,747] Trial 21 finished with value: 0.875625 and parameters: {'n_estimators': 176, 'learning_rate': 0.20098312793157194, 'max_depth': 5}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:54:37,525] Trial 22 finished with value: 0.8760416666666667 and parameters: {'n_estimators': 171, 'learning_rate': 0.23455488304192854, 'max_depth': 8}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:54:50,755] Trial 23 finished with value: 0.8758333333333334 and parameters: {'n_estimators': 139, 'learning_rate': 0.1423019346457341, 'max_depth': 7}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:55:01,659] Trial 24 finished with value: 0.875625 and parameters: {'n_estimators': 158, 'learning_rate': 0.2533419113882593, 'max_depth': 6}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:55:17,534] Trial 25 finished with value: 0.8758333333333334 and parameters: {'n_estimators': 125, 'learning_rate': 0.1844874863029768, 'max_depth': 8}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:55:55,124] Trial 26 finished with value: 0.8760416666666667 and parameters: {'n_estimators': 184, 'learning_rate': 0.10431875818276028, 'max_depth': 10}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:56:04,530] Trial 27 finished with value: 0.875625 and parameters: {'n_estimators': 198, 'learning_rate': 0.22007442628750268, 'max_depth': 5}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:56:31,234] Trial 28 finished with value: 0.8760416666666667 and parameters: {'n_estimators': 165, 'learning_rate': 0.19447438149646315, 'max_depth': 9}. Best is trial 0 with value: 0.8760416666666667.\n",
            "[I 2025-04-08 07:56:38,061] Trial 29 finished with value: 0.8758333333333334 and parameters: {'n_estimators': 75, 'learning_rate': 0.2596526021278954, 'max_depth': 7}. Best is trial 0 with value: 0.8760416666666667.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial for gb: {'n_estimators': 158, 'learning_rate': 0.157005251956179, 'max_depth': 8}\n",
            "\n",
            "Training GB...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-08 07:57:16,492] A new study created in memory with name: no-name-75182d17-ff49-4979-b60c-1091f8a4735b\n",
            "[I 2025-04-08 07:57:16,847] Trial 0 finished with value: 0.8133333333333334 and parameters: {'n_estimators': 89, 'learning_rate': 0.5864894040377142}. Best is trial 0 with value: 0.8133333333333334.\n",
            "[I 2025-04-08 07:57:17,305] Trial 1 finished with value: 0.8289583333333334 and parameters: {'n_estimators': 115, 'learning_rate': 0.5427497261768822}. Best is trial 1 with value: 0.8289583333333334.\n",
            "[I 2025-04-08 07:57:17,873] Trial 2 finished with value: 0.8310416666666667 and parameters: {'n_estimators': 143, 'learning_rate': 0.8571632350516035}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:18,629] Trial 3 finished with value: 0.8220833333333334 and parameters: {'n_estimators': 191, 'learning_rate': 0.5972819441776357}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:18,914] Trial 4 finished with value: 0.8175 and parameters: {'n_estimators': 75, 'learning_rate': 0.7007760668327061}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:19,775] Trial 5 finished with value: 0.830625 and parameters: {'n_estimators': 170, 'learning_rate': 0.698798699753315}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:20,437] Trial 6 finished with value: 0.8295833333333333 and parameters: {'n_estimators': 121, 'learning_rate': 0.8539950763159826}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:21,243] Trial 7 finished with value: 0.8120833333333334 and parameters: {'n_estimators': 136, 'learning_rate': 0.2819465145368855}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:21,902] Trial 8 finished with value: 0.8160416666666667 and parameters: {'n_estimators': 150, 'learning_rate': 0.1660910730861763}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:22,383] Trial 9 finished with value: 0.8216666666666667 and parameters: {'n_estimators': 120, 'learning_rate': 0.9511369263528974}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:23,029] Trial 10 finished with value: 0.8270833333333333 and parameters: {'n_estimators': 166, 'learning_rate': 0.9904240055762065}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:23,750] Trial 11 finished with value: 0.8291666666666667 and parameters: {'n_estimators': 184, 'learning_rate': 0.7656577004019869}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:24,389] Trial 12 finished with value: 0.8308333333333333 and parameters: {'n_estimators': 163, 'learning_rate': 0.43120222301225336}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:25,014] Trial 13 finished with value: 0.8260416666666667 and parameters: {'n_estimators': 157, 'learning_rate': 0.3375854930391903}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:25,820] Trial 14 finished with value: 0.8308333333333333 and parameters: {'n_estimators': 200, 'learning_rate': 0.3994201014718452}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:26,224] Trial 15 finished with value: 0.8154166666666667 and parameters: {'n_estimators': 100, 'learning_rate': 0.1403234584728354}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:26,783] Trial 16 finished with value: 0.8308333333333333 and parameters: {'n_estimators': 143, 'learning_rate': 0.4455726336523359}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:27,477] Trial 17 finished with value: 0.8260416666666667 and parameters: {'n_estimators': 174, 'learning_rate': 0.28964140037668773}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:27,702] Trial 18 finished with value: 0.8175 and parameters: {'n_estimators': 54, 'learning_rate': 0.8240102680540211}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:28,267] Trial 19 finished with value: 0.8191666666666667 and parameters: {'n_estimators': 138, 'learning_rate': 0.07660010912453419}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:28,894] Trial 20 finished with value: 0.8308333333333333 and parameters: {'n_estimators': 158, 'learning_rate': 0.47236556296427534}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:29,681] Trial 21 finished with value: 0.8308333333333333 and parameters: {'n_estimators': 199, 'learning_rate': 0.423823398249952}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:30,411] Trial 22 finished with value: 0.8308333333333333 and parameters: {'n_estimators': 182, 'learning_rate': 0.3350473788658212}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:31,209] Trial 23 finished with value: 0.8308333333333333 and parameters: {'n_estimators': 199, 'learning_rate': 0.39504102424957277}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:31,869] Trial 24 finished with value: 0.8227083333333334 and parameters: {'n_estimators': 131, 'learning_rate': 0.6532751908565466}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:32,867] Trial 25 finished with value: 0.8175 and parameters: {'n_estimators': 178, 'learning_rate': 0.2040170772614374}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:33,802] Trial 26 finished with value: 0.8308333333333333 and parameters: {'n_estimators': 160, 'learning_rate': 0.5219827279095605}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:34,390] Trial 27 finished with value: 0.8195833333333333 and parameters: {'n_estimators': 147, 'learning_rate': 0.25328170883241863}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:34,830] Trial 28 finished with value: 0.8304166666666667 and parameters: {'n_estimators': 111, 'learning_rate': 0.901279704670274}. Best is trial 2 with value: 0.8310416666666667.\n",
            "[I 2025-04-08 07:57:35,180] Trial 29 finished with value: 0.8133333333333334 and parameters: {'n_estimators': 85, 'learning_rate': 0.6230301771886197}. Best is trial 2 with value: 0.8310416666666667.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial for ada: {'n_estimators': 143, 'learning_rate': 0.8571632350516035}\n",
            "\n",
            "Training ADA...\n",
            "\n",
            "XGB Results:\n",
            "Accuracy: 0.8283, Precision: 0.7843, Recall: 0.8283, F1: 0.8006\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Disabled       0.00      0.00      0.00         7\n",
            "    Employed       0.84      0.93      0.88       774\n",
            "   Housework       0.33      0.10      0.16        77\n",
            "       Other       0.00      0.00      0.00        17\n",
            "     Retired       0.85      0.86      0.85       173\n",
            "     Student       0.94      0.94      0.94       120\n",
            "  Unemployed       0.18      0.09      0.12        32\n",
            "\n",
            "    accuracy                           0.83      1200\n",
            "   macro avg       0.45      0.42      0.42      1200\n",
            "weighted avg       0.78      0.83      0.80      1200\n",
            "\n",
            "\n",
            "RF Results:\n",
            "Accuracy: 0.8325, Precision: 0.7702, Recall: 0.8325, F1: 0.7919\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Disabled       0.00      0.00      0.00         7\n",
            "    Employed       0.83      0.95      0.89       774\n",
            "   Housework       0.25      0.03      0.05        77\n",
            "       Other       0.00      0.00      0.00        17\n",
            "     Retired       0.85      0.86      0.85       173\n",
            "     Student       0.93      0.94      0.93       120\n",
            "  Unemployed       0.14      0.03      0.05        32\n",
            "\n",
            "    accuracy                           0.83      1200\n",
            "   macro avg       0.43      0.40      0.40      1200\n",
            "weighted avg       0.77      0.83      0.79      1200\n",
            "\n",
            "\n",
            "SVM Results:\n",
            "Accuracy: 0.8417, Precision: 0.7844, Recall: 0.8417, F1: 0.7964\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Disabled       0.00      0.00      0.00         7\n",
            "    Employed       0.83      0.96      0.89       774\n",
            "   Housework       0.50      0.03      0.05        77\n",
            "       Other       0.00      0.00      0.00        17\n",
            "     Retired       0.83      0.87      0.85       173\n",
            "     Student       0.95      0.97      0.96       120\n",
            "  Unemployed       0.00      0.00      0.00        32\n",
            "\n",
            "    accuracy                           0.84      1200\n",
            "   macro avg       0.45      0.40      0.39      1200\n",
            "weighted avg       0.78      0.84      0.80      1200\n",
            "\n",
            "\n",
            "LOGREG Results:\n",
            "Accuracy: 0.8417, Precision: 0.7862, Recall: 0.8417, F1: 0.7971\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Disabled       0.00      0.00      0.00         7\n",
            "    Employed       0.83      0.96      0.89       774\n",
            "   Housework       0.50      0.03      0.05        77\n",
            "       Other       0.00      0.00      0.00        17\n",
            "     Retired       0.86      0.86      0.86       173\n",
            "     Student       0.95      0.96      0.95       120\n",
            "  Unemployed       0.00      0.00      0.00        32\n",
            "\n",
            "    accuracy                           0.84      1200\n",
            "   macro avg       0.45      0.40      0.39      1200\n",
            "weighted avg       0.79      0.84      0.80      1200\n",
            "\n",
            "\n",
            "GB Results:\n",
            "Accuracy: 0.8158, Precision: 0.7762, Recall: 0.8158, F1: 0.7913\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Disabled       0.00      0.00      0.00         7\n",
            "    Employed       0.84      0.92      0.88       774\n",
            "   Housework       0.31      0.10      0.16        77\n",
            "       Other       0.00      0.00      0.00        17\n",
            "     Retired       0.83      0.84      0.83       173\n",
            "     Student       0.92      0.94      0.93       120\n",
            "  Unemployed       0.17      0.09      0.12        32\n",
            "\n",
            "    accuracy                           0.82      1200\n",
            "   macro avg       0.44      0.41      0.42      1200\n",
            "weighted avg       0.78      0.82      0.79      1200\n",
            "\n",
            "\n",
            "ADA Results:\n",
            "Accuracy: 0.8400, Precision: 0.7485, Recall: 0.8400, F1: 0.7906\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Disabled       0.00      0.00      0.00         7\n",
            "    Employed       0.83      0.97      0.89       774\n",
            "   Housework       0.00      0.00      0.00        77\n",
            "       Other       0.00      0.00      0.00        17\n",
            "     Retired       0.88      0.86      0.87       173\n",
            "     Student       0.89      0.92      0.90       120\n",
            "  Unemployed       0.00      0.00      0.00        32\n",
            "\n",
            "    accuracy                           0.84      1200\n",
            "   macro avg       0.37      0.39      0.38      1200\n",
            "weighted avg       0.75      0.84      0.79      1200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and evaluate stacked model\n",
        "stacked_model = model.build_stacked_model()\n",
        "stk_pred = model.predict_stacked(stacked_model, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmv5rrFHV3G9",
        "outputId": "c190fcff-775e-4354-f3e1-fc50b371d9fb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STACKED MODEL RESULTS\n",
            "Accuracy: 0.8433\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Disabled       0.00      0.00      0.00         7\n",
            "    Employed       0.83      0.96      0.89       774\n",
            "   Housework       0.40      0.03      0.05        77\n",
            "       Other       0.00      0.00      0.00        17\n",
            "     Retired       0.85      0.87      0.86       173\n",
            "     Student       0.96      0.97      0.96       120\n",
            "  Unemployed       0.00      0.00      0.00        32\n",
            "\n",
            "    accuracy                           0.84      1200\n",
            "   macro avg       0.43      0.40      0.39      1200\n",
            "weighted avg       0.78      0.84      0.80      1200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display model accuracy results\n",
        "acc_df = pd.DataFrame(list(results.items()), columns=['Model', 'Accuracy'])\n",
        "acc_df = acc_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
        "print(\"\\n Model Accuracy Comparison Table:\")\n",
        "print(acc_df.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3dqFcenWU2M",
        "outputId": "6336e410-eaad-4226-f92b-7787c0d66509"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Model Accuracy Comparison Table:\n",
            " Model  Accuracy\n",
            "logreg  0.841667\n",
            "   svm  0.841667\n",
            "   ada  0.840000\n",
            "    rf  0.832500\n",
            "   xgb  0.828333\n",
            "    gb  0.815833\n"
          ]
        }
      ]
    }
  ]
}